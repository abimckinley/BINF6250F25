---
title: "Project 02"
author: "Abi McKinley and Tiange Feng"
date: "`r Sys.Date()`"
output:
  github_document:
    html_preview: false
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
library('reticulate')
use_python('/usr/bin/python3')
```

## Train Markov model

For our initial implementation of the Markov Model, we will use the simple example of Dr. Seuss: "One fish two fish red fish blue fish."

```{python}
import numpy as np
```

```{python}
def build_markov_model(markov_model, new_text):
    
    # Split input into words
    words = new_text.strip().split()
    
    # Add artificial start (*S*) and end (*E*) states
    padded_words = ["*S*"] + words + ["*E*"]
    
    # Loop through each word and its next word
    for i in range(len(padded_words) - 1):
        current_word = padded_words[i]
        next_word = padded_words[i + 1]
        
        # If current word not in model, add it
        if current_word not in markov_model:
            markov_model[current_word] = {}
        
        # If next_word not in current_word's dict, add it
        if next_word not in markov_model[current_word]:
            markov_model[current_word][next_word] = 0
        
        # Increment transition count
        markov_model[current_word][next_word] += 1
    
    return markov_model


# Example usage
markov_model = dict()
text = "one fish two fish red fish blue fish"
markov_model = build_markov_model(markov_model, text)
print(markov_model)
```

### Nth order Markov chain

```{python}
def build_markov_model(markov_model, text, order=1):
    
    # Split input text into words
    words = text.strip().split()
    
    # Pad with artificial start tokens and add an end token
    padded_words = ["*S*"] * order + words + ["*E*"]
    
    # Loop through words to build states and transitions
    for i in range(len(padded_words) - order):
        # Current state is a tuple of 'order' words
        state = tuple(padded_words[i : i + order])
        # Next word after the state
        next_word = padded_words[i + order]
        
        # If state is not in model, initialize dictionary
        if state not in markov_model:
            markov_model[state] = {}
        
        # If next_word not in state transitions, initialize count
        if next_word not in markov_model[state]:
            markov_model[state][next_word] = 0
        
        # Increment transition count
        markov_model[state][next_word] += 1
    
    return markov_model
```

## Generate text from Markov Model

Markov models are "generative models". That is, the probability states in the model can be used to generate output following the conditional probabilities in the model.

We will now generate a sequence of text from the Markov model. For this section, I recommend using np.random.choice, which allows for you to provide a probability distribution for drawing the next edge in the chain.

```{python}
def get_next_word(current_state, markov_model, seed=42):
 
    np.random.seed(seed)  # Set seed
    
    # Get possible next words and their counts
    next_words = list(markov_model[current_state].keys())
    counts = list(markov_model[current_state].values())
    
    # Convert counts to probabilities
    total = sum(counts)
    probabilities = [c / total for c in counts]
    
    # Choose next word based on distribution
    next_word = np.random.choice(next_words, p=probabilities)
    
    return next_word
```

```{python}
def generate_random_text(markov_model, order=1, seed=42):

    np.random.seed(seed)  # Fix randomness
    
    # Start from artificial start state
    current_state = tuple(["*S*"] * order)
    output = []
    
    while True:
        # Get next word from model
        next_word = get_next_word(current_state, markov_model, seed=np.random.randint(0, 10000))
        
        # Stop if we hit end token
        if next_word == "*E*":
            break
        
        # Append word to output
        output.append(next_word)
        
        # Update state (shift window forward by one word)
        current_state = tuple(list(current_state[1:]) + [next_word])
    
    # Join words into a sentence
    return " ".join(output)
```

---

## All the Fish

Up till now, you have only been working with a line or two of the Dr. Seuss' _One Fish, Two Fish_. Now, I want you to build a model using the whole book and try different orders of Markov models.

```{python}
def train_from_file(filename, order=1):
  with open(filename, 'r') as f:
    text = f.read()
    
# Initialize empty model
    markov_model = {}
    
    # Build the model from text
    markov_model = build_markov_model(markov_model, text, order)
    
    return markov_model
```

```{python}

def allthefish_model(str_start, order, seed = 7):

  markov_model = {
    '*S*': {},
    '*E*': {}
  }

  with open('one_fish_two_fish.txt', 'r') as file:

    for line in file:
      markov_model = build_markov_model(markov_model, line, order)

print (generate_random_text(markov_model, order=2, seed=7))
```

---

## Shakespeare

Now, let's play around with some Shakespeare.

```{python}
sonnet_markov_model = {}
with open("sonnets.txt", "r") as file:
    sonnet = ""
    for line in file:
        line = line.strip()
        if line == "":
            # Empty line means one sonnet ended, build model from it
            sonnet_markov_model = build_markov_model(sonnet_markov_model, sonnet, order=2)
            sonnet = ""
        else:
            sonnet = sonnet + " " + line
    # Handle last sonnet if file doesnâ€™t end with blank line
    if sonnet:
        sonnet_markov_model = build_markov_model(sonnet_markov_model, sonnet, order=2)

print(generate_random_text(sonnet_markov_model, order=2, seed=7))
```
